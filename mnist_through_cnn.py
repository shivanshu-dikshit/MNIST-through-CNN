# -*- coding: utf-8 -*-
"""MNIST through CNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BBna0aDW4yQt5Qo-kG2RyFW0gE9TkKOA
"""

#libraries for further use if needed
import numpy as np
import pandas as pd

import tensorflow as tf
tf.test.gpu_device_name()

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets("/tmp/data/", one_hot = True)
#loading the mnist data set from tensor flow

num_classes = 10 #from  0 - 9
batch_size = 128
learning_rate= 0.001

mnist.train.images.shape

mnist.train.labels.shape

x = tf.placeholder(tf.float32, shape = [None, 784])
y = tf.placeholder(tf.float32)

def conv2d(x, w):
  return tf.nn.conv2d(x, w, strides = [1,1,1,1], padding = 'SAME')

def maxpool2d(x):
  return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')

#defining the waits and neural net
#filter size of 3x3
def neural_net(x):
  weights = {'w_layer1':tf.Variable(tf.random_normal([3,3,1,32])),
             'w_layer2':tf.Variable(tf.random_normal([3,3,32,64])),
             'w_fc_layer':tf.Variable(tf.random_normal([7*7*64, 1024])),
             'w_out_layer':tf.Variable(tf.random_normal([1024, num_classes]))}
  
  bias = {'b_layer1':tf.Variable(tf.random_normal([32])),
          'b_layer2':tf.Variable(tf.random_normal([64])),
          'b_fc_layer':tf.Variable(tf.random_normal([1024])),
          'b_out_layer':tf.Variable(tf.random_normal([num_classes]))}
  
  x = tf.reshape(x, shape = [-1, 28, 28, 1])
  
  layer1 = tf.nn.relu(conv2d(x, weights['w_layer1'] + bias['b_layer1']))
  layer1_pool = maxpool2d(layer1)
  
  layer2 = tf.nn.relu(conv2d(layer1_pool, weights['w_layer2'] + bias['b_layer2']))
  layer2_pool = maxpool2d(layer2)
  
  fc_reshape = tf.reshape(layer2_pool, shape = [-1, 7*7*64])
  fc = tf.nn.relu(tf.matmul(fc_reshape, weights['w_fc_layer']) + bias['b_fc_layer'])
  
  output = tf.matmul(fc, weights['w_out_layer']) + bias['b_out_layer']
  
  return output

def train_neural_net(x):
  
  result = neural_net(x)
  cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = result))
  optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)
  
  
  hm_epochs = 100
  with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())
    for epoch in range(hm_epochs):
            epoch_loss = 0
            for _ in range(int(mnist.train.num_examples/batch_size)):
                epoch_x, epoch_y = mnist.train.next_batch(batch_size)
                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})
                epoch_loss += c

            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)

            correct = tf.equal(tf.argmax(result, 1), tf.argmax(y, 1))

            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))
            print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))

train_neural_net(x)



"""## Epoch 99 completed out of 100 loss: 196.56430632977663
## ***Accuracy***: 0.9889
## 98% **accurac**y ****
"""

